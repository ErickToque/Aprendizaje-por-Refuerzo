{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOiL95qUg+H7A9LPrEYsIoC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Caso generico"],"metadata":{"id":"599wK8tLXuQT"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W6mByoXdTBX-","executionInfo":{"status":"ok","timestamp":1755036039482,"user_tz":300,"elapsed":142,"user":{"displayName":"Erick Saul Toque Encinas","userId":"07377281190749278830"}},"outputId":"5ec9b69b-4843-4b85-e27b-4414403313a8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Tabla Q final:\n","[[ 0.          7.33901672  0.        ]\n"," [ 0.          0.          5.70104457]\n"," [ 0.          6.42663646  0.        ]\n"," [-0.1         7.0239987   0.        ]\n"," [ 0.          5.98365536  0.118     ]\n"," [ 0.          6.76282459  0.        ]\n"," [-0.1         0.          5.93697511]\n"," [-0.064342    7.01500636  0.        ]\n"," [ 0.          6.51251366  0.12173952]\n"," [ 0.          6.4429814   0.        ]\n"," [ 0.          0.          5.36035246]\n"," [ 0.          0.          4.89208927]\n"," [-0.091       0.          4.52897726]\n"," [ 0.          5.06721422  0.1       ]\n"," [-0.1         0.          4.26665869]\n"," [-0.1         4.8399159   0.        ]\n"," [-0.091       0.          4.50925444]\n"," [-0.082       5.73506884  0.        ]\n"," [-0.1         5.76632026  0.        ]\n"," [-0.04505463  0.          5.23498066]\n"," [ 0.          5.98488272  0.        ]\n"," [ 0.          6.16627896  0.        ]\n"," [ 0.          5.4743797   0.1       ]\n"," [ 0.          0.          4.10606027]\n"," [ 0.          0.26622632  4.299631  ]\n"," [-0.05122     5.73281362  0.        ]\n"," [ 0.          5.23598539  0.        ]\n"," [-0.0461737   5.55161184  0.        ]\n"," [ 0.          5.9898659   0.        ]\n"," [-0.1         5.5347145   0.        ]\n"," [ 0.          5.85864201  0.1       ]\n"," [-0.1         5.22065301  0.109     ]\n"," [ 0.          5.50220049  0.        ]\n"," [ 0.          0.4477502   5.09746755]\n"," [-0.1         6.03955197  0.        ]\n"," [ 0.          6.08342276  0.1       ]\n"," [ 0.          0.2         5.20292023]\n"," [ 0.          5.73040858  0.        ]\n"," [-0.1         6.49137757  0.        ]\n"," [ 0.          0.          4.51144032]\n"," [-0.1         6.20982454  0.        ]\n"," [ 0.          0.          5.00027221]\n"," [ 0.          5.58062574  0.199     ]\n"," [ 0.          5.9433195   0.        ]\n"," [-0.0658      0.          4.53407749]\n"," [-0.082       6.05328544  0.23021003]\n"," [ 0.          6.63726003  0.        ]\n"," [-0.08639841  0.          5.82315449]\n"," [ 0.          6.95851694  0.        ]\n"," [ 0.          6.52490901  0.        ]\n"," [ 0.          7.0447851   0.        ]\n"," [ 0.          7.37454763  0.        ]\n"," [-0.1         0.28521636  6.14007313]\n"," [ 0.          7.20994378  0.        ]\n"," [-0.17767268  6.81317034  0.        ]\n"," [ 0.          0.          5.60921809]\n"," [-0.03207907  0.          6.15465409]\n"," [-0.1         6.44426516  0.        ]\n"," [-0.1         0.          4.83221845]\n"," [-0.1         0.          5.142691  ]\n"," [-0.1         0.          5.43777521]\n"," [-0.1         0.23772357  5.66403264]\n"," [ 0.          6.84441798  0.        ]\n"," [-0.1         0.          6.14496302]\n"," [ 0.          7.10255079  0.        ]\n"," [ 0.          7.01478633  0.        ]\n"," [-0.1         6.54765619  0.1       ]\n"," [-0.1         0.          5.84248727]\n"," [ 0.          6.86000499  0.        ]\n"," [-0.1         0.          5.56314015]\n"," [ 0.          6.36925569  0.        ]\n"," [ 0.          0.          5.20304401]\n"," [ 0.          0.          4.91740985]\n"," [ 0.          5.9924276   0.        ]\n"," [ 0.          0.          4.50531075]\n"," [-0.1729      5.6438662   0.        ]\n"," [-0.1         0.          5.01404775]\n"," [-0.1         0.          4.90058416]\n"," [ 0.          6.19480171  0.1       ]\n"," [ 0.          6.84712499  0.        ]\n"," [ 0.          0.          5.26030174]\n"," [ 0.          0.          5.36025426]\n"," [ 0.          5.67172633  0.        ]\n"," [ 0.          5.65803467  0.        ]\n"," [-0.0367858   0.          5.20765989]\n"," [ 0.          0.          5.5432548 ]\n"," [ 0.          0.          5.59198226]\n"," [-0.08236267  6.47226575  0.        ]\n"," [-0.1         6.92014617  0.        ]\n"," [ 0.          0.          5.4857785 ]\n"," [-0.16163598  7.43489112  0.1       ]\n"," [ 0.          6.91116695  0.1351558 ]\n"," [ 0.          0.          6.22254746]\n"," [ 0.          7.07659345  0.1       ]\n"," [ 0.          7.03903514  0.        ]\n"," [-0.1         7.09302308  0.        ]\n"," [ 0.          7.26837317  0.1       ]\n"," [ 0.          0.          6.55354058]\n"," [ 0.          0.          6.67563087]\n"," [ 0.          7.11770236  0.        ]]\n"]}],"source":["import numpy as np\n","\n","# Estados: [frecuencia cardiaca, glucosa]\n","# Acciones: 0 = no hacer nada, 1 = dar medicación, 2 = recomendar ejercicio\n","# Recompensa: mejora en las métricas\n","\n","n_states = 100  # discretización de posibles estados\n","n_actions = 3\n","Q = np.zeros((n_states, n_actions))\n","\n","# Simulación\n","for episode in range(500):\n","    state = np.random.randint(0, n_states)\n","    done = False\n","\n","    while not done:\n","        action = np.argmax(Q[state] + np.random.randn(1, n_actions) * 0.1)\n","\n","        # Simulamos transición y recompensa\n","        if action == 1:  # medicación\n","            reward = 2\n","        elif action == 2:  # ejercicio\n","            reward = 1\n","        else:\n","            reward = -1\n","\n","        next_state = (state + np.random.randint(-3, 3)) % n_states\n","\n","        # Actualización Q-learning\n","        Q[state, action] += 0.1 * (reward + 0.9 * np.max(Q[next_state]) - Q[state, action])\n","\n","        state = next_state\n","        done = np.random.rand() < 0.1  # 10% de probabilidad de terminar\n","\n","print(\"Tabla Q final:\")\n","print(Q)\n"]},{"cell_type":"markdown","source":["# Caso diabetes"],"metadata":{"id":"VUxgKmf7XyrJ"}},{"cell_type":"code","source":["import numpy as np\n","\n","# Parámetros del entorno\n","niveles_glucosa = np.linspace(50, 300, 26)  # 26 estados (mg/dL)\n","acciones = [-2, -1, 0, 1, 2]  # Cambio en dosis (-2 = mucho menos, 2 = mucho más)\n","n_states = len(niveles_glucosa)\n","n_actions = len(acciones)\n","\n","# Tabla Q\n","Q = np.zeros((n_states, n_actions))\n","\n","# Función de recompensa\n","def recompensa(glucosa):\n","    if 80 <= glucosa <= 120:  # rango óptimo\n","        return 10\n","    elif 60 <= glucosa <= 140:\n","        return 5\n","    else:\n","        return -10\n","\n","# Simulación de entrenamiento\n","for episodio in range(200):\n","    estado = np.random.randint(0, n_states)  # glucosa inicial\n","    glucosa_actual = niveles_glucosa[estado]\n","    terminado = False\n","\n","    while not terminado:\n","        # Política ε-greedy\n","        if np.random.rand() < 0.1:\n","            accion_idx = np.random.randint(0, n_actions)\n","        else:\n","            accion_idx = np.argmax(Q[estado])\n","\n","        cambio_dosis = acciones[accion_idx]\n","\n","        # Simulación de efecto en glucosa\n","        glucosa_siguiente = glucosa_actual - cambio_dosis * np.random.randint(5, 15)\n","        glucosa_siguiente = np.clip(glucosa_siguiente, 50, 300)\n","\n","        # Obtener nuevo estado y recompensa\n","        nuevo_estado = np.argmin(abs(niveles_glucosa - glucosa_siguiente))\n","        r = recompensa(glucosa_siguiente)\n","\n","        # Actualización Q-learning\n","        Q[estado, accion_idx] += 0.1 * (r + 0.9 * np.max(Q[nuevo_estado]) - Q[estado, accion_idx])\n","\n","        estado = nuevo_estado\n","        glucosa_actual = glucosa_siguiente\n","\n","        # Condición de finalización\n","        terminado = np.random.rand() < 0.05  # fin de episodio aleatorio\n","\n","# Política aprendida\n","politica = [acciones[np.argmax(Q[s])] for s in range(n_states)]\n","\n","print(\"Política aprendida (cambio de dosis por nivel de glucosa):\")\n","for g, a in zip(niveles_glucosa, politica):\n","    print(f\"Glucosa {g:.0f} mg/dL -> cambio de dosis {a}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sqtrHq7mX7uS","executionInfo":{"status":"ok","timestamp":1755037287871,"user_tz":300,"elapsed":78,"user":{"displayName":"Erick Saul Toque Encinas","userId":"07377281190749278830"}},"outputId":"f68cdd11-087b-4eab-ced8-cf21a4e53bac"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Política aprendida (cambio de dosis por nivel de glucosa):\n","Glucosa 50 mg/dL -> cambio de dosis -2\n","Glucosa 60 mg/dL -> cambio de dosis -2\n","Glucosa 70 mg/dL -> cambio de dosis -2\n","Glucosa 80 mg/dL -> cambio de dosis -2\n","Glucosa 90 mg/dL -> cambio de dosis -2\n","Glucosa 100 mg/dL -> cambio de dosis -2\n","Glucosa 110 mg/dL -> cambio de dosis 0\n","Glucosa 120 mg/dL -> cambio de dosis -1\n","Glucosa 130 mg/dL -> cambio de dosis 1\n","Glucosa 140 mg/dL -> cambio de dosis 1\n","Glucosa 150 mg/dL -> cambio de dosis 2\n","Glucosa 160 mg/dL -> cambio de dosis 2\n","Glucosa 170 mg/dL -> cambio de dosis 2\n","Glucosa 180 mg/dL -> cambio de dosis 2\n","Glucosa 190 mg/dL -> cambio de dosis 2\n","Glucosa 200 mg/dL -> cambio de dosis 2\n","Glucosa 210 mg/dL -> cambio de dosis -1\n","Glucosa 220 mg/dL -> cambio de dosis 2\n","Glucosa 230 mg/dL -> cambio de dosis -1\n","Glucosa 240 mg/dL -> cambio de dosis 1\n","Glucosa 250 mg/dL -> cambio de dosis -1\n","Glucosa 260 mg/dL -> cambio de dosis 1\n","Glucosa 270 mg/dL -> cambio de dosis -1\n","Glucosa 280 mg/dL -> cambio de dosis -1\n","Glucosa 290 mg/dL -> cambio de dosis 0\n","Glucosa 300 mg/dL -> cambio de dosis 2\n"]}]},{"cell_type":"markdown","source":["#  Control de ventilador mecánico con Q-Learning"],"metadata":{"id":"GkPG8n9GmrcP"}},{"cell_type":"code","source":["import numpy as np\n","\n","# Parámetros\n","n_states = 21  # Saturación de oxígeno: 80% a 100% (21 estados)\n","n_actions = 3  # 0: bajar volumen, 1: mantener, 2: subir volumen\n","\n","Q = np.zeros((n_states, n_actions))\n","\n","alpha = 0.1      # tasa aprendizaje\n","gamma = 0.9      # factor descuento\n","epsilon = 0.1    # exploración\n","\n","def get_reward(spo2):\n","    if 90 <= spo2 <= 100:\n","        return 10  # buen estado\n","    elif spo2 < 90:\n","        return -10  # peligro\n","    else:\n","        return -5  # saturación demasiado alta\n","\n","def next_spo2(current_spo2, action):\n","    # Simulación simplificada: acción afecta saturación con algo de ruido\n","    if action == 0:\n","        change = -1 + np.random.randn() * 0.5\n","    elif action == 1:\n","        change = np.random.randn() * 0.5\n","    else:\n","        change = 1 + np.random.randn() * 0.5\n","    new_spo2 = current_spo2 + change\n","    return int(np.clip(new_spo2, 80, 100))\n","\n","episodes = 1000\n","\n","for ep in range(episodes):\n","    state = np.random.randint(0, n_states)  # saturación inicial\n","    done = False\n","    while not done:\n","        if np.random.rand() < epsilon:\n","            action = np.random.choice(n_actions)\n","        else:\n","            action = np.argmax(Q[state])\n","\n","        spo2 = 80 + state  # valor real de saturación\n","        reward = get_reward(spo2)\n","        next_state = next_spo2(spo2, action) - 80\n","\n","        Q[state, action] += alpha * (reward + gamma * np.max(Q[next_state]) - Q[state, action])\n","        state = next_state\n","\n","        # Condición de finalización arbitraria\n","        done = reward == 10\n","\n","print(\"Entrenamiento finalizado.\")\n","\n","# Política aprendida (ejemplo para saturación de 85 a 95)\n","for spo2_level in range(65, 96):\n","    state = spo2_level - 80\n","    action = np.argmax(Q[state])\n","    action_str = {0: \"Bajar volumen\", 1: \"Mantener volumen\", 2: \"Subir volumen\"}[action]\n","    print(f\"Saturación {spo2_level}% -> Acción: {action_str}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Kq2iQNfdmsr9","executionInfo":{"status":"ok","timestamp":1755041182620,"user_tz":300,"elapsed":210,"user":{"displayName":"Erick Saul Toque Encinas","userId":"07377281190749278830"}},"outputId":"f158c58b-128b-4b72-f2e8-ee8fe8d92571"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Entrenamiento finalizado.\n","Saturación 65% -> Acción: Subir volumen\n","Saturación 66% -> Acción: Subir volumen\n","Saturación 67% -> Acción: Subir volumen\n","Saturación 68% -> Acción: Subir volumen\n","Saturación 69% -> Acción: Subir volumen\n","Saturación 70% -> Acción: Subir volumen\n","Saturación 71% -> Acción: Bajar volumen\n","Saturación 72% -> Acción: Bajar volumen\n","Saturación 73% -> Acción: Bajar volumen\n","Saturación 74% -> Acción: Bajar volumen\n","Saturación 75% -> Acción: Bajar volumen\n","Saturación 76% -> Acción: Bajar volumen\n","Saturación 77% -> Acción: Bajar volumen\n","Saturación 78% -> Acción: Bajar volumen\n","Saturación 79% -> Acción: Bajar volumen\n","Saturación 80% -> Acción: Subir volumen\n","Saturación 81% -> Acción: Subir volumen\n","Saturación 82% -> Acción: Subir volumen\n","Saturación 83% -> Acción: Subir volumen\n","Saturación 84% -> Acción: Subir volumen\n","Saturación 85% -> Acción: Subir volumen\n","Saturación 86% -> Acción: Subir volumen\n","Saturación 87% -> Acción: Subir volumen\n","Saturación 88% -> Acción: Subir volumen\n","Saturación 89% -> Acción: Subir volumen\n","Saturación 90% -> Acción: Subir volumen\n","Saturación 91% -> Acción: Subir volumen\n","Saturación 92% -> Acción: Bajar volumen\n","Saturación 93% -> Acción: Bajar volumen\n","Saturación 94% -> Acción: Bajar volumen\n","Saturación 95% -> Acción: Bajar volumen\n"]}]},{"cell_type":"markdown","source":["# Optimización de asignación de citas médicas con Q-Learning"],"metadata":{"id":"Aoxa8Z_7nDYl"}},{"cell_type":"code","source":["import numpy as np\n","\n","# Estados y acciones\n","n_states = 11   # pacientes en espera de 0 a 10\n","n_actions = 4   # agendar de 0 a 3 pacientes\n","\n","Q = np.zeros((n_states, n_actions))\n","\n","alpha = 0.1\n","gamma = 0.95\n","epsilon = 0.1\n","\n","def reward(waiting, action):\n","    # Penalizar si se sobrecarga (más de 10 pacientes en cola)\n","    next_waiting = waiting + action - 2  # 2 pacientes atendidos por bloque\n","    if next_waiting < 0:\n","        next_waiting = 0\n","    if next_waiting > 10:\n","        return -10\n","    # Recompensa mayor si la espera se reduce\n","    return -next_waiting\n","\n","def next_state(current, action):\n","    next_waiting = current + action - 2\n","    if next_waiting < 0:\n","        next_waiting = 0\n","    if next_waiting > 10:\n","        next_waiting = 10\n","    return next_waiting\n","\n","episodes = 500\n","\n","for ep in range(episodes):\n","    state = np.random.randint(0, n_states)\n","    done = False\n","    steps = 0\n","    while not done and steps < 20:\n","        if np.random.rand() < epsilon:\n","            action = np.random.randint(0, n_actions)\n","        else:\n","            action = np.argmax(Q[state])\n","\n","        r = reward(state, action)\n","        ns = next_state(state, action)\n","\n","        Q[state, action] += alpha * (r + gamma * np.max(Q[ns]) - Q[state, action])\n","\n","        state = ns\n","        steps += 1\n","\n","print(\"Entrenamiento finalizado.\")\n","\n","# Mostrar política aprendida\n","for waiting in range(n_states):\n","    best_action = np.argmax(Q[waiting])\n","    print(f\"Pacientes en espera: {waiting} -> Agendar pacientes: {best_action}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X78tn_fqnEjm","executionInfo":{"status":"ok","timestamp":1755041273173,"user_tz":300,"elapsed":140,"user":{"displayName":"Erick Saul Toque Encinas","userId":"07377281190749278830"}},"outputId":"b29674aa-21f9-493c-e14b-ea82bf7afa40"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Entrenamiento finalizado.\n","Pacientes en espera: 0 -> Agendar pacientes: 0\n","Pacientes en espera: 1 -> Agendar pacientes: 0\n","Pacientes en espera: 2 -> Agendar pacientes: 0\n","Pacientes en espera: 3 -> Agendar pacientes: 0\n","Pacientes en espera: 4 -> Agendar pacientes: 0\n","Pacientes en espera: 5 -> Agendar pacientes: 0\n","Pacientes en espera: 6 -> Agendar pacientes: 0\n","Pacientes en espera: 7 -> Agendar pacientes: 0\n","Pacientes en espera: 8 -> Agendar pacientes: 0\n","Pacientes en espera: 9 -> Agendar pacientes: 0\n","Pacientes en espera: 10 -> Agendar pacientes: 0\n"]}]},{"cell_type":"markdown","source":["# Control de presión arterial con Q-Learning"],"metadata":{"id":"TEkPzX7inPjl"}},{"cell_type":"code","source":["import numpy as np\n","\n","# Estados: 0=muy baja, 1=baja, 2=normal, 3=alta, 4=muy alta\n","n_states = 5\n","# Acciones: 0=disminuir dosis, 1=mantener dosis, 2=aumentar dosis\n","n_actions = 3\n","\n","Q = np.zeros((n_states, n_actions))\n","\n","alpha = 0.1\n","gamma = 0.9\n","epsilon = 0.1\n","\n","def reward(state):\n","    # Recompensa alta si está en estado normal (2)\n","    if state == 2:\n","        return 10\n","    else:\n","        return -abs(2 - state) * 5  # penaliza lo lejos del normal\n","\n","def next_state(state, action):\n","    # Simula efecto de la acción\n","    if action == 0:  # disminuir dosis baja presión\n","        return max(0, state - 1)\n","    elif action == 2:  # aumentar dosis sube presión\n","        return min(n_states - 1, state + 1)\n","    else:\n","        # Mantener dosis puede causar pequeña variación aleatoria\n","        return min(max(0, state + np.random.choice([-1,0,1])), n_states - 1)\n","\n","episodes = 1000\n","\n","for ep in range(episodes):\n","    state = np.random.randint(0, n_states)\n","    done = False\n","    steps = 0\n","    while not done and steps < 20:\n","        if np.random.rand() < epsilon:\n","            action = np.random.randint(0, n_actions)\n","        else:\n","            action = np.argmax(Q[state])\n","\n","        r = reward(state)\n","        ns = next_state(state, action)\n","\n","        Q[state, action] += alpha * (r + gamma * np.max(Q[ns]) - Q[state, action])\n","\n","        state = ns\n","        steps += 1\n","        if steps >= 20:\n","            done = True\n","\n","print(\"Entrenamiento finalizado.\")\n","\n","# Mostrar política aprendida\n","for s in range(n_states):\n","    best_action = np.argmax(Q[s])\n","    action_name = [\"Disminuir dosis\", \"Mantener dosis\", \"Aumentar dosis\"][best_action]\n","    print(f\"Estado presión arterial: {s} -> Acción recomendada: {action_name}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DtqPLqNmnQ3k","executionInfo":{"status":"ok","timestamp":1755041312927,"user_tz":300,"elapsed":674,"user":{"displayName":"Erick Saul Toque Encinas","userId":"07377281190749278830"}},"outputId":"e9f80bef-8b15-4f61-99fb-9e8eff5f4db8"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Entrenamiento finalizado.\n","Estado presión arterial: 0 -> Acción recomendada: Aumentar dosis\n","Estado presión arterial: 1 -> Acción recomendada: Aumentar dosis\n","Estado presión arterial: 2 -> Acción recomendada: Mantener dosis\n","Estado presión arterial: 3 -> Acción recomendada: Disminuir dosis\n","Estado presión arterial: 4 -> Acción recomendada: Disminuir dosis\n"]}]}]}